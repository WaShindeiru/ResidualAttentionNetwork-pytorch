{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:06.059300Z",
     "start_time": "2025-06-17T17:35:06.043389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "d94b2fc5ae045c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:08.744618Z",
     "start_time": "2025-06-17T17:35:06.134576Z"
    }
   },
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import metrics"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 19:35:07.811508: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 19:35:07.820442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750181707.830071  302215 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750181707.833262  302215 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750181707.842325  302215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750181707.842334  302215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750181707.842335  302215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750181707.842337  302215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-17 19:35:07.844964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.632178Z",
     "start_time": "2025-06-17T17:35:08.948058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "c585664ad201d215",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.700289Z",
     "start_time": "2025-06-17T17:35:09.662132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data_mnist_test',\n",
    "    train=False,\n",
    "    download=True\n",
    ")"
   ],
   "id": "6f10b228fcfac17a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.773871Z",
     "start_time": "2025-06-17T17:35:09.715281Z"
    }
   },
   "cell_type": "code",
   "source": "device = ('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "9a8730b1634e79c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.820045Z",
     "start_time": "2025-06-17T17:35:09.787404Z"
    }
   },
   "cell_type": "code",
   "source": "print(device)",
   "id": "7d31a83e20952327",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.871641Z",
     "start_time": "2025-06-17T17:35:09.839379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_to_dictionary(data):\n",
    "    keys_ = data[0].keys()\n",
    "    length_ = len(data)\n",
    "\n",
    "    temp_ = {}\n",
    "    for key_ in keys_:\n",
    "        temp_[key_] = []\n",
    "        for i in range(length_):\n",
    "            temp_[key_].append(data[i][key_])\n",
    "\n",
    "    return temp_"
   ],
   "id": "76ba71d0fba258a2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN",
   "id": "1941782d7710351b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:09.924200Z",
     "start_time": "2025-06-17T17:35:09.892167Z"
    }
   },
   "cell_type": "code",
   "source": "from CNN.cnn import CNN_",
   "id": "dad631376d79879a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.057177Z",
     "start_time": "2025-06-17T17:35:09.943507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_cnn = CNN_().to(device)\n",
    "model_cnn.load_state_dict(torch.load(\"./CNN/weights/CNN_2025-06-17_18:37:22.pth\", weights_only=True))\n",
    "model_cnn.eval()\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ],
   "id": "177d5e194a672243",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.102646Z",
     "start_time": "2025-06-17T17:35:10.070638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"CNN/results/CNN_2025-06-17_18:37:22/history.pickle\"\n",
    "with open(path, 'rb') as f:\n",
    "    data_cnn = pickle.load(f)"
   ],
   "id": "c67c923355d27d1c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN LSTM",
   "id": "e7b00f1d0759594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.153662Z",
     "start_time": "2025-06-17T17:35:10.122087Z"
    }
   },
   "cell_type": "code",
   "source": "from LSTM.lstm import cnn_lstm_",
   "id": "4d27d8004b583238",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.217178Z",
     "start_time": "2025-06-17T17:35:10.172198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_lstm = cnn_lstm_().to(device)\n",
    "model_lstm.load_state_dict(torch.load(\"LSTM/weights/CNN_lstm_2025-06-17_18:45:11.pth\", weights_only=True))\n",
    "model_lstm.eval()\n",
    "\n",
    "transform_lstm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ],
   "id": "ac6cd10d99cab27f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.261579Z",
     "start_time": "2025-06-17T17:35:10.230038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"LSTM/results/CNN_lstm_2025-06-17_18:45:11/history.pickle\"\n",
    "with open(path, 'rb') as f:\n",
    "    data_lstm = pickle.load(f)"
   ],
   "id": "43242c596c5a1c0c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ResNet + attention",
   "id": "cf06d2d4e9e45aff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.513681Z",
     "start_time": "2025-06-17T17:35:10.281498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ResNet.model.residual_attention_network import ResidualAttentionModel_92_32input_update as ResidualAttentionModel\n",
    "\n",
    "model_res = ResidualAttentionModel().to(device)\n",
    "model_res.load_state_dict(torch.load('./ResNet/weights/ResidualAttentionModel_92_32input_update_2025-06-16_23:37:33.pth', weights_only=True))\n",
    "model_res.eval()\n",
    "\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Pad(padding=2),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "id": "7d2a0cd0730c9ba1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.559862Z",
     "start_time": "2025-06-17T17:35:10.527604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"./ResNet/results/ResidualAttentionModel_92_32input_update_2025-06-16_23:37:33/history.pickle\"\n",
    "with open(path, 'rb') as f:\n",
    "    data_res = pickle.load(f)"
   ],
   "id": "5e827d17a77e05ce",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.611754Z",
     "start_time": "2025-06-17T17:35:10.580249Z"
    }
   },
   "cell_type": "code",
   "source": "data_res = list_to_dictionary(data_res)",
   "id": "c088639811f8dbac",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VGG + attention",
   "id": "d5bb1666e8e81997"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:10.664169Z",
     "start_time": "2025-06-17T17:35:10.631372Z"
    }
   },
   "cell_type": "code",
   "source": "from VGG.networks import AttnVGG",
   "id": "54ec0e000ddc3b4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.494334Z",
     "start_time": "2025-06-17T17:35:10.683707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_vgg = AttnVGG(num_classes=10).to(device)\n",
    "model_vgg.load_state_dict(torch.load(\"./VGG/weights/VGG_attention2025-06-17_13:42:24.pth\", weights_only=True))\n",
    "model_vgg.eval()\n",
    "\n",
    "transform_vgg = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "id": "c9e188676438818e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.539568Z",
     "start_time": "2025-06-17T17:35:11.507690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"./VGG/results/VGG_attention2025-06-17_13:42:24/history.pickle\"\n",
    "with open(path, 'rb') as f:\n",
    "    data_vgg = pickle.load(f)"
   ],
   "id": "8a0026ff28a6fd19",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.591615Z",
     "start_time": "2025-06-17T17:35:11.558787Z"
    }
   },
   "cell_type": "code",
   "source": "data_vv = list_to_dictionary(data_vgg)",
   "id": "8d27a076240509f4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vision Transformer",
   "id": "de613249d1d40968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.645848Z",
     "start_time": "2025-06-17T17:35:11.611696Z"
    }
   },
   "cell_type": "code",
   "source": "from VisionTransformer.simple_vit import SimpleViT",
   "id": "6de35b4fe5d94f1d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.748379Z",
     "start_time": "2025-06-17T17:35:11.663875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_sm_vt = SimpleViT(image_size=224, patch_size=16, num_classes=10, dim=128, depth=6, heads=16, mlp_dim=2048).to(device)\n",
    "model_sm_vt.load_state_dict(torch.load(\"./VisionTransformer/weights/VT_simple_2025-06-17_18:07:55.pth\", weights_only=True))\n",
    "model_sm_vt.eval()\n",
    "\n",
    "transform_sm_vt = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "id": "94580725f72eedad",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.798629Z",
     "start_time": "2025-06-17T17:35:11.764453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"./VisionTransformer/results/VT_simple_2025-06-17_18:07:55/history.pickle\"\n",
    "with open(path, 'rb') as f:\n",
    "    data_sm_vt = pickle.load(f)"
   ],
   "id": "b72625aa0cb9908b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.856871Z",
     "start_time": "2025-06-17T17:35:11.823262Z"
    }
   },
   "cell_type": "code",
   "source": "data_sm_vt = list_to_dictionary(data_sm_vt)",
   "id": "d7c926bf27a60266",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Porównanie",
   "id": "9a907b44298da80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.906232Z",
     "start_time": "2025-06-17T17:35:11.874815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_num = 32\n",
    "\n",
    "x = [test_dataset[i][0] for i in range(16)]\n",
    "y = [test_dataset[i][1] for i in range(16)]"
   ],
   "id": "83a5dacc7b0cfcf4",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:11.957555Z",
     "start_time": "2025-06-17T17:35:11.925587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compare_dict = {\n",
    "    'CNN': {'model': model_cnn, 'transform': transform_cnn, 'history': data_cnn},\n",
    "    'LSTM': {'model': model_lstm, 'transform': transform_lstm, 'history': data_lstm},\n",
    "    'ResNet': {'model': model_res, 'transform': transform_resnet, 'history': data_res},\n",
    "    'VGG': {'model': model_vgg, 'transform': transform_vgg, 'history': data_vgg},\n",
    "    'sm_vt': {'model': model_sm_vt, 'transform': transform_sm_vt, 'history': data_sm_vt},\n",
    "}"
   ],
   "id": "f6669cf47cd63d1b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:35:47.367431Z",
     "start_time": "2025-06-17T17:35:47.268223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import metrics\n",
    "importlib.reload(metrics)\n",
    "metrics.compare_models(compare_dict, x, y, device)"
   ],
   "id": "93d7c268eeac1003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Rozpoczynanie analizy modeli ---\n",
      "Analizowanie: CNN...\n",
      "(16,)\n",
      "(16,)\n",
      "⚠️ Ostrzeżenie: Nie udało się obliczyć FLOPS. Błąd: 'CNN_' object has no attribute 'inputs'\n",
      "Analizowanie: LSTM...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmetrics\u001B[39;00m\n\u001B[32m      3\u001B[39m importlib.reload(metrics)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mmetrics\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompare_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/studia/sem_8/ssn/projekt_v2/ResidualAttentionNetwork-pytorch/metrics.py:69\u001B[39m, in \u001B[36mcompare_models\u001B[39m\u001B[34m(models_dict, x_test, y_test_cat, device)\u001B[39m\n\u001B[32m     67\u001B[39m history = data[\u001B[33m'\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     68\u001B[39m transform_ = data[\u001B[33m'\u001B[39m\u001B[33mtransform\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m x_transformed = [\u001B[43mtransform_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x_ \u001B[38;5;129;01min\u001B[39;00m x_test]\n\u001B[32m     70\u001B[39m x_test = torch.stack(x_transformed).to(device)\n\u001B[32m     72\u001B[39m start_time = time.time()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.venv/data_torch/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.venv/data_torch/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001B[39m, in \u001B[36mToTensor.__call__\u001B[39m\u001B[34m(self, pic)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[32m    130\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m \u001B[33;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[32m    136\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.venv/data_torch/lib/python3.12/site-packages/torchvision/transforms/functional.py:142\u001B[39m, in \u001B[36mto_tensor\u001B[39m\u001B[34m(pic)\u001B[39m\n\u001B[32m    140\u001B[39m     _log_api_usage_once(to_tensor)\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil._is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy(pic) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[32m    145\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic.ndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m dimensions.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
